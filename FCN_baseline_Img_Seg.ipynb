{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCN_baseline_Img_Seg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philqc/Deep-Value-Networks-Pytorch/blob/master/FCN_baseline_Img_Seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WwSE_tJAGvEr",
        "colab_type": "code",
        "outputId": "6a8fda21-12c2-46cd-f3f7-b6cc3975c332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms.functional as TF\n",
        "import numbers\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import pdb\n",
        "from skimage import io"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 753.6MB 4.7MB/s \n",
            "\u001b[31mfastai 1.0.50.post1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MxsJs2SaUxbo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For some reason, we are getting AttributeError: module 'PIL.Image' has no attribute 'register_extensions if we don't install pillow version 4.1.1"
      ]
    },
    {
      "metadata": {
        "id": "eR3Ddub5UvWH",
        "colab_type": "code",
        "outputId": "1f4d92dd-3a18-4e18-9053-28cc7100187e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pillow==4.1.1\n",
        "%reload_ext autoreload\n",
        "%autoreload"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow==4.1.1 in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow==4.1.1) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SmxcaxcyJK6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ]
    },
    {
      "metadata": {
        "id": "sOlkh_0KJIXm",
        "colab_type": "code",
        "outputId": "cf173ab7-dca5-456b-a1b4-9c44fdf629f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# directory path to store results/plots/models\n",
        "dir_path = 'drive/My Drive/projet_asp'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kVVBr6lYHZJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Auxiliary functions"
      ]
    },
    {
      "metadata": {
        "id": "sV6i9KA3HVjG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.inputs[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "\n",
        "def show_img(img, black_and_white=True):\n",
        "    np_img = img.numpy()\n",
        "    # put channel at the end for plt.imshow\n",
        "    if np_img.ndim == 3:\n",
        "        np_img = np.transpose(np_img, (1, 2, 0))\n",
        "\n",
        "    #print('np_img.shape', np_img.shape)\n",
        "    if black_and_white:\n",
        "        plt.imshow(np_img, cmap='Greys_r')\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.imshow(np_img)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def save_img(img, path_to_save, black_and_white=True):\n",
        "    np_img = img.numpy()\n",
        "    np_img = np.transpose(np_img, (1, 2, 0))\n",
        "    if black_and_white:\n",
        "        plt.imsave(path_to_save + \".jpg\", np_img, cmap='Greys_r')\n",
        "    else:\n",
        "        plt.imsave(path_to_save + \".jpg\", np_img)\n",
        "\n",
        "\n",
        "def save_grid_imgs(input_imgs, path_to_save, black_and_white=True):\n",
        "    img = utils.make_grid(input_imgs, nrow=8)\n",
        "    save_img(img, path_to_save, black_and_white)\n",
        "    \n",
        "def show_grid_imgs(input_imgs, black_and_white=True):\n",
        "    img = utils.make_grid(input_imgs, nrow=8)  \n",
        "    show_img(img, black_and_white)\n",
        "\n",
        "\n",
        "def plot_results(results, iou):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    ----------\n",
        "    results: dictionary with the train/valid loss\n",
        "    and the f1 scores]\n",
        "    iou: bool\n",
        "      if true: print IOU, else print F1 Score\n",
        "    \"\"\"\n",
        "    str_score = 'IOU' if iou else 'F1 Score'\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    ax1.set_title('Validation Loss')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.set_xlabel('epochs')\n",
        "    ax2.set_title('Validation ' + str_score)\n",
        "    ax2.set_ylabel(str_score)\n",
        "    ax2.set_xlabel('epochs')\n",
        "\n",
        "    ax1.plot(results['loss_train'], label='loss_train')\n",
        "    ax1.plot(results['loss_valid'], label='loss_valid')\n",
        "    if iou:\n",
        "      ax2.plot(results['IOU_valid'])\n",
        "    else:\n",
        "      ax2.plot(results['f1_valid'])\n",
        "      \n",
        "    ax1.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdrrDwXwJeyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Value Network"
      ]
    },
    {
      "metadata": {
        "id": "_C0qykNVKqiL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cross_entropy2d(inputs, target, weight=None, size_average=False):\n",
        "    \"\"\"https://github.com/RohanDoshi2018/pytorch-fcn/blob/master/torchfcn/trainer.py\"\"\"\n",
        "    # input: (n, c, h, w), target: (n, h, w)\n",
        "    n, c, h, w = inputs.size()\n",
        "    target = target.view(n, h, w)\n",
        "    #print('n ={}; c = {}; h = {}; w ={}'.format(n, c, h, w))\n",
        "    #print('inputs.size() =', inputs.size(), 'target.size() =', target.size())\n",
        "    # log_p: (n, c, h, w)\n",
        "    log_p = F.log_softmax(inputs, dim=1)\n",
        "    # log_p: (n*h*w, c)\n",
        "    log_p = log_p.permute(0, 2, 3, 1).contiguous()\n",
        "    log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0]\n",
        "    log_p = log_p.view(-1, c)\n",
        "    # target: (n*h*w,)\n",
        "    mask = target >= 0\n",
        "    target = target[mask]\n",
        "    loss = F.nll_loss(log_p, target.long(), weight=weight, reduction='sum')\n",
        "    if size_average:\n",
        "        loss /= mask.data.sum()\n",
        "    return loss\n",
        "\n",
        "def thirtysix_crop(img, size):\n",
        "    \"\"\" Crop the given PIL Image 32x32 into 36 crops of 24x24\n",
        "    Inspired from five_crop implementation in pytorch\n",
        "    https://pytorch.org/docs/master/_modules/torchvision/transforms/functional.html\n",
        "    \"\"\"\n",
        "    if isinstance(size, numbers.Number):\n",
        "        size = (int(size), int(size))\n",
        "    else:\n",
        "        assert len(size) == 2, \"Please provide only two dimensions (h, w) for size.\"\n",
        "        \n",
        "    w, h = img.size\n",
        "    crop_h, crop_w = size\n",
        "    if crop_w > w or crop_h > h:\n",
        "        raise ValueError(\"Requested crop size {} is bigger than input size {}\"\n",
        "                         \"\".format(size, (h, w)))\n",
        "    crops = []    \n",
        "    for i in [0, 2, 3, 4, 5, 8]:\n",
        "        for j in [0, 2, 3, 4, 5, 8]:\n",
        "            c = img.crop((i, j, crop_w + i, crop_h + j))\n",
        "            if c.size != size:\n",
        "                raise ValueError(\"Crop size is {} but should be {}\"\n",
        "                                 \"\".format( c.size, size))\n",
        "            crops.append(c)\n",
        "               \n",
        "    return crops\n",
        "    \n",
        "    \n",
        "def average_over_crops(crops, device):\n",
        "    N, n_crops, n_class, h, w = crops.shape\n",
        "    pred = crops.permute(0, 1, 3, 4, 2).contiguous().view(-1, n_class).argmax(dim=1).view(N, n_crops, h, w)\n",
        "        \n",
        "    final = torch.zeros(N, 32, 32).to(device)\n",
        "    size = torch.zeros(32, 32).to(device)\n",
        "    for i, x in enumerate([0, 2, 3, 4, 5, 8]):\n",
        "        for j, y in enumerate([0, 2, 3, 4, 5, 8]):\n",
        "            k = i * 6 + j\n",
        "            #pdb.set_trace()\n",
        "            final[:, x: w + x, y:h + y] += pred[:, k].float()\n",
        "            size[x: w + x, y:h + y] += 1\n",
        "    \n",
        "    final /= size\n",
        "    final = final.view(N, 1, 32, 32)\n",
        "    #pdb.set_trace()\n",
        "    return final \n",
        "\n",
        "  \n",
        "class FCNBaseLine(nn.Module):\n",
        "\n",
        "    def __init__(self, non_linearity='relu', use_batch_norm=False):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5, 1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 5, 2, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 128, 5, 2, padding=2)\n",
        "\n",
        "        # Taken from https://openreview.net/forum?id=By40DoAqtX\n",
        "        #self.deconv1 = nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1)\n",
        "        #self.deconv2 = nn.ConvTranspose2d(1, 1, kernel_size=8, stride=4, padding=14)\n",
        "\n",
        "        # Mine\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 2, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(2, 2, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(64) if use_batch_norm else None\n",
        "        self.bn2 = nn.BatchNorm2d(128) if use_batch_norm else None\n",
        "        self.bn3 = nn.BatchNorm2d(128) if use_batch_norm else None\n",
        "        self.bn4 = nn.BatchNorm2d(2) if use_batch_norm else None\n",
        "\n",
        "        non_linearity = non_linearity.lower()\n",
        "        if non_linearity == 'softplus':\n",
        "            self.non_linearity = nn.Softplus()\n",
        "        elif non_linearity == 'relu':\n",
        "            self.non_linearity = nn.ReLU()\n",
        "        elif non_linearity == 'elu':\n",
        "            self.non_linearity = nn.ELU()\n",
        "        elif non_linearity == 'tanh':\n",
        "            self.non_linearity = nn.Tanh()\n",
        "        else:\n",
        "            raise ValueError('Unknown activation Convnet:', non_linearity)\n",
        "         \n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #print('(0) z.shape=', z.shape)\n",
        "        if self.use_batch_norm:\n",
        "            z = self.non_linearity(self.bn1(self.conv1(z)))\n",
        "            #print('(1) z.shape=', z.shape)\n",
        "            z = self.non_linearity(self.bn2(self.conv2(z)))\n",
        "            #print('(2) z.shape=', z.shape)\n",
        "            z = self.non_linearity(self.bn3(self.conv3(z)))\n",
        "            #print('(3) z.shape=', z.shape)\n",
        "            z = self.non_linearity(self.bn4(self.deconv1(z)))\n",
        "            #print('(4) z.shape=', z.shape)\n",
        "            z = self.deconv2(z)\n",
        "        else:\n",
        "            z = self.non_linearity(self.conv1(z))\n",
        "            z = self.non_linearity(self.conv2(z))\n",
        "            z = self.non_linearity(self.conv3(z))\n",
        "            z = self.non_linearity(self.deconv1(z))\n",
        "            #z = self.dropout(z)\n",
        "            z = self.deconv2(z)\n",
        "\n",
        "        #print('(5) z.shape=', z.shape)\n",
        "        return z  # size = (N, n_class, x.H/1, x.W/1)\n",
        "\n",
        "      \n",
        "\n",
        "class FullyConvNet:\n",
        "    def __init__(self, train_loader, valid_loader, dir_path, use_cuda,\n",
        "                 non_linearity='relu', learning_rate=1e-3, feature_dim=(24, 24), label_dim=(24, 24)):\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "        self.feature_dim = feature_dim\n",
        "        self.label_dim = label_dim\n",
        "\n",
        "        # Using standard FCN\n",
        "        self.model = FCNBaseLine(non_linearity, use_batch_norm=True).to(self.device)\n",
        "\n",
        "        # Binary Cross entropy loss\n",
        "        # Computes independent loss for each label in the vector\n",
        "        # Our final loss is the sum over all our losses\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "        # Hyperparameters from LDRSP paper\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3, \n",
        "                                          weight_decay=1e-4)\n",
        "\n",
        "        self.n_class = 2       \n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.n_train = len(train_loader.dataset)\n",
        "        self.n_valid = len(valid_loader.dataset)\n",
        "        print('Using a {} train {} validation split'.format(self.n_train, self.n_valid))\n",
        "        self.batch_size = train_loader.batch_size\n",
        "        self.batch_size_eval = valid_loader.batch_size\n",
        "        \n",
        "        # turn on/off\n",
        "        self.training = False\n",
        "\n",
        "    def get_oracle_value(self, pred_labels, gt_labels):\n",
        "        \"\"\"\n",
        "        Compute the ground truth value, i.e. v*(y, y*)\n",
        "        of some predicted labels, where v*(y, y*)\n",
        "        is the relaxed version of the IOU (intersection\n",
        "        over union) when training, and the discrete IOU\n",
        "        when validating/testing\n",
        "        \"\"\"\n",
        "        if pred_labels.shape != gt_labels.shape:\n",
        "            raise ValueError('Invalid labels shape: gt = ', gt_labels.shape, 'pred = ', pred_labels.shape)\n",
        "\n",
        "        if not self.training:\n",
        "            # No relaxation, 0-1 only\n",
        "            pred_labels = torch.where(pred_labels >= 0.5,\n",
        "                                      torch.ones(1).to(self.device),\n",
        "                                      torch.zeros(1).to(self.device))\n",
        "            pred_labels = pred_labels.float()\n",
        "\n",
        "        pred_labels = torch.flatten(pred_labels).reshape(pred_labels.size()[0], -1)\n",
        "        gt_labels = torch.flatten(gt_labels).reshape(gt_labels.size()[0], -1)\n",
        "\n",
        "        intersect = torch.min(pred_labels, gt_labels)\n",
        "        union = torch.max(pred_labels, gt_labels)\n",
        "\n",
        "        # for numerical stability\n",
        "        epsilon = torch.full(union.size(), 10 ** -8).to(self.device)\n",
        "\n",
        "        f1 = 2 * intersect / (intersect + torch.max(epsilon, union))\n",
        "        # we want a (Batch_size x 1) tensor\n",
        "        #iou = iou.view(-1, 1)\n",
        "        #pdb.set_trace()\n",
        "        return f1\n",
        "      \n",
        "    def get_iou_value(self, pred_labels, gt_labels):\n",
        "        \"\"\"\n",
        "        Compute the ground truth value, i.e. v*(y, y*)\n",
        "        of some predicted labels, where v*(y, y*)\n",
        "        is the relaxed version of the IOU (intersection\n",
        "        over union) when training, and the discrete IOU\n",
        "        when validating/testing\n",
        "        \"\"\"\n",
        "        if pred_labels.shape != gt_labels.shape:\n",
        "            raise ValueError('Invalid labels shape: gt = ', gt_labels.shape, 'pred = ', pred_labels.shape)\n",
        "\n",
        "        if not self.training:\n",
        "            # No relaxation, 0-1 only\n",
        "            pred_labels = torch.where(pred_labels >= 0.5,\n",
        "                                      torch.ones(1).to(self.device),\n",
        "                                      torch.zeros(1).to(self.device))\n",
        "            pred_labels = pred_labels.float()\n",
        "\n",
        "        pred_labels = torch.flatten(pred_labels).reshape(pred_labels.size()[0], -1)\n",
        "        gt_labels = torch.flatten(gt_labels).reshape(gt_labels.size()[0], -1)\n",
        "\n",
        "        intersect = torch.sum(torch.min(pred_labels, gt_labels), dim=1)\n",
        "        union = torch.sum(torch.max(pred_labels, gt_labels), dim=1)\n",
        "\n",
        "        # for numerical stability\n",
        "        epsilon = torch.full(union.size(), 10 ** -8).to(self.device)\n",
        "        \n",
        "        iou = intersect / torch.max(epsilon, union)\n",
        "        # we want a (Batch_size x 1) tensor\n",
        "        iou = iou.view(-1, 1)\n",
        "        #pdb.set_trace()\n",
        "        return iou\n",
        "\n",
        "    def train(self, ep):\n",
        "\n",
        "        self.model.train()\n",
        "        self.training = True\n",
        "\n",
        "        time_start = time.time()\n",
        "        t_loss, t_size = 0, 0\n",
        "        \n",
        "        for batch_idx, (raw_inputs, inputs, targets) in enumerate(self.train_loader):\n",
        "\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "            inputs, targets = inputs.float(), targets.float()\n",
        "            \n",
        "            t_size += len(inputs)\n",
        "\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            output = self.model(inputs)\n",
        "            \n",
        "            loss = cross_entropy2d(output, targets)           \n",
        "            t_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            # Get prediction accross (argmax accross n_class depth)\n",
        "            N, c, h, w = output.shape\n",
        "            pred = output.permute(0, 2, 3, 1).contiguous().view(-1, c).argmax(dim=1).view(N, 1, h, w)\n",
        "                        \n",
        "            if batch_idx % 2 == 0:\n",
        "                print('\\rTraining Epoch {} [{} / {} ({:.0f}%)]: Time per epoch: {:.2f}s; '\n",
        "                      'Avg_Loss = {:.5f}; IOU_batch = {:.2f}%'\n",
        "                      ''.format(ep, t_size, self.n_train, 100 * t_size / self.n_train,\n",
        "                               (self.n_train / t_size) * (time.time() - time_start), t_loss / t_size,\n",
        "                               100 * self.get_iou_value(pred.float(), targets).mean()),\n",
        "                      end='')\n",
        "        \n",
        "        if ep % 50 == 0:\n",
        "            print('\\nTRAIN ')\n",
        "            img = raw_inputs.detach().cpu()\n",
        "            show_grid_imgs(img)\n",
        "               \n",
        "            mask = pred.detach().cpu()\n",
        "            # Put mask to float otherwise only outputs black images !!\n",
        "            show_grid_imgs(mask.float())   \n",
        "            print('---------------------------------------')\n",
        "            \n",
        "        t_loss /= t_size\n",
        "        self.training = False\n",
        "        print('')\n",
        "        return t_loss\n",
        "\n",
        "    def valid(self, loader, test_set=False, ep=0):\n",
        "\n",
        "        self.model.eval()\n",
        "        self.training = False\n",
        "\n",
        "        loss, t_size = 0, 0\n",
        "        mean_iou = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for (raw_inputs, inputs, targets) in loader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                inputs, targets = inputs.float(), targets.float()\n",
        "            \n",
        "                t_size += len(inputs)\n",
        "                \n",
        "                # For valid/test: inputs is a 5d tensor\n",
        "                bs, ncrops, channels, h, w = inputs.size()\n",
        "                \n",
        "                output = self.model(inputs.view(-1, channels, h, w)) # fuse batch size and ncrops\n",
        "                # go back to normal shape\n",
        "                output = output.view(bs, ncrops, self.n_class, h, w)\n",
        "                \n",
        "                pred = average_over_crops(output, self.device)\n",
        "                #output_avg = output.view(bs, ncrops, -1).mean(1) # avg over crops\n",
        "                \n",
        "                # go back to normal shape\n",
        "                #output_avg = output_avg.view(bs, -1, h, w)\n",
        "                \n",
        "                #loss += cross_entropy2d(output_avg, targets)\n",
        "                \n",
        "                # Get prediction accross (argmax accross n_class depth)\n",
        "                #N, n_class, h, w = output_avg.shape\n",
        "                #pred = output_avg.permute(0, 2, 3, 1).contiguous().view(-1, n_class).argmax(dim=1).view(N, 1, h, w)\n",
        "                \n",
        "                mean_iou.append(self.get_iou_value(pred.float(), targets).mean()) \n",
        "                \n",
        "        if ep % 50 == 0:\n",
        "            img = raw_inputs.detach().cpu()\n",
        "            show_grid_imgs(img) \n",
        "            mask = pred.detach().cpu()\n",
        "            mask = mask >= 0.5\n",
        "            # Put mask to float otherwise only outputs black images !!\n",
        "            show_grid_imgs(mask.float())        \n",
        "        \n",
        "        mean_iou = torch.stack(mean_iou)\n",
        "        mean_iou = torch.mean(mean_iou)\n",
        "        mean_iou = mean_iou.cpu().numpy()\n",
        "        loss /= t_size\n",
        "\n",
        "        str_first = 'Test set' if test_set else 'Validation set'\n",
        "        print('{}: IOU = {:.2f}%'\n",
        "              ''.format(str_first, 100 * mean_iou))\n",
        "\n",
        "        return loss, mean_iou\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIG7bXZ9TeLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Functions to run epoch and also to run hyperparameter search"
      ]
    },
    {
      "metadata": {
        "id": "VYLkf9ZfTgcp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_fcn_model(train_loader, valid_loader, dir_path, use_cuda, n_epochs):\n",
        "\n",
        "    FCN = FullyConvNet(train_loader, valid_loader, dir_path, use_cuda)\n",
        "\n",
        "    results = {'name': 'FCN_Whorse', 'loss_train': [],\n",
        "               'loss_valid': [], 'IOU_valid': [], 'batch_size': train_loader.batch_size,\n",
        "               'batch_size_eval': valid_loader.batch_size}\n",
        "\n",
        "    results_path = dir_path + '/results/'\n",
        "    if not os.path.isdir(results_path):\n",
        "        os.makedirs(results_path)\n",
        "\n",
        "    # Increment a counter so that previous results with the same args will not\n",
        "    # be overwritten. Comment out the next four lines if you only want to keep\n",
        "    # the most recent results.\n",
        "    i = 0\n",
        "    while os.path.exists(results_path + str(i) + '.pkl'):\n",
        "        i += 1\n",
        "    results_path = results_path + str(i)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        loss_train = FCN.train(epoch)\n",
        "        loss_valid, iou_valid = FCN.valid(FCN.valid_loader, ep=epoch)\n",
        "        results['loss_train'].append(loss_train)\n",
        "        results['loss_valid'].append(loss_valid)\n",
        "        results['IOU_valid'].append(iou_valid)\n",
        "\n",
        "        with open(results_path + '.pkl', 'wb') as fout:\n",
        "            pickle.dump(results, fout)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5pWSv5OWqMu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load dataset and choose to use cpu or gpu"
      ]
    },
    {
      "metadata": {
        "id": "RBWt5LiCWnhc",
        "colab_type": "code",
        "outputId": "d9b77fb8-fbd5-470c-9167-1e0c8fd33488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# build the dataset, generate the \"training tuple\"\n",
        "class WeizmannHorseDataset(Dataset):\n",
        "    \"\"\" Weizmann Horse Dataset \"\"\"\n",
        "    \n",
        "    def __init__(self, img_dir, mask_dir, subset='train', transform=None,\n",
        "                 random_mirroring=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir(string): Path to the image file (training image)\n",
        "            mask_dir(string): Path to the mask file (segmentation result)\n",
        "            subset(string): 'train' or 'valid' or 'test'\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "\n",
        "        all_img_names = os.listdir(img_dir)\n",
        "        all_mask_names = os.listdir(mask_dir)\n",
        "\n",
        "        self.img_names = []\n",
        "        self.mask_names = []\n",
        "        for i, name in enumerate(all_img_names):\n",
        "            img_number = ''.join([n for n in name if n.isdigit()])\n",
        "            if int(img_number) >= 200:\n",
        "                if subset == 'test':\n",
        "                    self.img_names.append(name)\n",
        "                    self.mask_names.append(all_mask_names[i])\n",
        "            elif 180 <= int(img_number) < 200:\n",
        "                if subset == 'valid':\n",
        "                    self.img_names.append(name)\n",
        "                    self.mask_names.append(all_mask_names[i])\n",
        "            elif subset == 'train' and int(img_number) < 180:\n",
        "                self.img_names.append(name)\n",
        "                self.mask_names.append(all_mask_names[i])\n",
        "        \n",
        "        assert len(self.mask_names) == len(self.img_names)\n",
        "        \n",
        "        self.is_test = True if subset == 'valid' or subset == 'test' else False\n",
        "        \n",
        "        self.transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                             transforms.Resize(size=(32, 32))])\n",
        "        self.random_mirroring = random_mirroring\n",
        "        self.normalize = None\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        \n",
        "        self.transform_test = transforms.Compose([transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "                                                  transforms.Lambda(lambda crops: torch.stack([self.normalize(crop) for crop in crops]))])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.img_names[idx])\n",
        "        mask_name = os.path.join(self.mask_dir, self.mask_names[idx])\n",
        "        \n",
        "        image = io.imread(img_name)\n",
        "        mask = io.imread(mask_name)\n",
        "               \n",
        "        if self.transform:\n",
        "                      \n",
        "            image = self.transform(image)\n",
        "            \n",
        "            # create a channel for mask so as to transform\n",
        "            mask = self.transform(np.expand_dims(mask, axis=2))\n",
        "            \n",
        "            if self.is_test:\n",
        "                input_img = image\n",
        "                # Use 36 crops averaging for valid/test\n",
        "                image = thirtysix_crop(image, 24)\n",
        "                image = self.transform_test(image)\n",
        "            else:\n",
        "                # Random crop\n",
        "                i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(24, 24))\n",
        "                image = TF.crop(image, i, j, h, w)                \n",
        "                mask = TF.crop(mask, i, j, h, w)\n",
        "                \n",
        "                # Random Horizontal flipping\n",
        "                if self.random_mirroring and random.random() > 0.50:\n",
        "                    image = TF.hflip(image)\n",
        "                    mask = TF.hflip(mask)\n",
        "                \n",
        "                input_img = image\n",
        "                image = self.to_tensor(image)\n",
        "                image = self.normalize(image)\n",
        "            \n",
        "            input_img = self.to_tensor(input_img)      \n",
        "            mask = self.to_tensor(mask)\n",
        "            \n",
        "            # put mask to 0-1 again\n",
        "            mask = mask >= 0.5\n",
        "        \n",
        "        return input_img, image, mask\n",
        "\n",
        "    def compute_mean_and_stddev(self):\n",
        "        n_images = len(self.img_names)\n",
        "        masks, images = [], []\n",
        "\n",
        "        # ToTensor transforms the images/masks in range [0, 1]\n",
        "        transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                        transforms.Resize(size=(32, 32)),\n",
        "                                        transforms.ToTensor()])\n",
        "\n",
        "        for i in range(n_images):\n",
        "            mask_name = os.path.join(self.mask_dir, self.mask_names[i])\n",
        "            img_name = os.path.join(self.img_dir, self.img_names[i])\n",
        "            mask = io.imread(mask_name)\n",
        "            image = io.imread(img_name)\n",
        "\n",
        "            image = transform(image)\n",
        "            # create a channel for mask so as to transform\n",
        "            mask = transform(np.expand_dims(mask, axis=2))\n",
        "\n",
        "            masks.append(mask)\n",
        "            images.append(image)\n",
        "\n",
        "        # after torch.stack, we should have n_images x 1 x 32 x 32 for mask\n",
        "        # and have n_images x 3 x 32 x 32 for images\n",
        "        images, masks = torch.stack(images), torch.stack(masks)\n",
        "\n",
        "        # compute mean and std_dev of images\n",
        "        # put the images in n_images x 3 x (32x32) shape\n",
        "        images = images.view(images.size(0), images.size(1), -1)\n",
        "        mean_imgs = images.mean(2).sum(0) / n_images\n",
        "        std_imgs = images.std(2).sum(0) / n_images\n",
        "        ############################################\n",
        "\n",
        "        # Find mean_mask for visualization purposes\n",
        "        height, width = masks.shape[2], masks.shape[3]\n",
        "        # flatten\n",
        "        masks = masks.view(n_images, 1, -1)\n",
        "        mean_mask = torch.mean(masks, dim=0)\n",
        "        # go back to 32 x 32 view\n",
        "        mean_mask = mean_mask.view(1, 1, height, width)\n",
        "\n",
        "        print_mask = False\n",
        "        if print_mask:\n",
        "            img_to_show = mean_mask.squeeze(0)\n",
        "            img_to_show = img_to_show.squeeze(0)\n",
        "            show_img(img_to_show, black_and_white=True)\n",
        "\n",
        "        return mean_imgs, std_imgs, mean_mask\n",
        "\n",
        "# Use GPU if it is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "image_dir = dir_path + '/images'\n",
        "mask_dir = dir_path + '/masks'\n",
        "\n",
        "# Use Dataset to resize and convert to Tensor\n",
        "train_set = WeizmannHorseDataset(image_dir, mask_dir, subset='train',\n",
        "                                 transform=None, random_mirroring=True)\n",
        "\n",
        "valid_set = WeizmannHorseDataset(image_dir, mask_dir, subset='valid',\n",
        "                                 transform=None, random_mirroring=True)\n",
        "\n",
        "batch_size = 20\n",
        "batch_size_eval = 10\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_set,\n",
        "    batch_size=batch_size_eval,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "mean_imgs, std_imgs, mean_mask = train_set.compute_mean_and_stddev()\n",
        "print('mean_imgs =', mean_imgs, 'std_dev_imgs =', std_imgs)\n",
        "\n",
        "train_set.normalize = transforms.Normalize(mean_imgs, std_imgs)\n",
        "valid_set.normalize = transforms.Normalize(mean_imgs, std_imgs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_imgs = tensor([0.6131, 0.5283, 0.4726]) std_dev_imgs = tensor([0.2131, 0.2110, 0.2025])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H6Dehg2TWrVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Run the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "izCw-KAgG6-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Fully convnet model\n",
        "run_fcn_model(train_loader, valid_loader, dir_path, use_cuda, n_epochs=2000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}